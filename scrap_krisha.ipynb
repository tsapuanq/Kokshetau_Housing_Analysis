{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2193a1aa-83cf-4c05-a972-7dd43423241f",
   "metadata": {},
   "source": [
    "# PARSING FROM 'KRISHA.KZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38eb59-34fa-42a7-ba33-10b7626fb28f",
   "metadata": {},
   "source": [
    "### 0 step. Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39233b-1ddc-4d53-ade2-69d206e9f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0725c1-26c3-4b67-bd83-2eabf0f73468",
   "metadata": {},
   "source": [
    "### 1 step. Opens the Chrome by the selenium and loads the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c7a88-4786-4dbb-b35b-f825c04ed8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://krisha.kz/prodazha/kvartiry/kokshetau/\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde91b8-30c2-42cb-a17d-2cee82114a7f",
   "metadata": {},
   "source": [
    "### 2 step. Parsing all necessary links and name of streets from ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f12932-e4ca-435e-83b6-1f8357da63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "links = []\n",
    "streets = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    a_elements = soup.findAll(\"a\", class_=\"a-card__title\")\n",
    "    subtitle_elements = soup.findAll(\"div\", class_=\"a-card__subtitle\")\n",
    "    \n",
    "    for a_element in a_elements:\n",
    "        link = a_element.get('href')\n",
    "        links.append(link)\n",
    "        \n",
    "    print(f\"Number of links: {len(links)}\")\n",
    "    \n",
    "    for subtitle in subtitle_elements:\n",
    "        street = subtitle.get_text(strip=True) \n",
    "        streets.append(street)\n",
    "        \n",
    "    print(f\"Number of streets: {len(links)}\")\n",
    "    \n",
    "    try:\n",
    "        next_button = driver.find_element(By.LINK_TEXT, \"Дальше\")\n",
    "        next_button.click() \n",
    "        \n",
    "    except:\n",
    "        print(\"Button 'Дальше' not found, loop is stopped\")\n",
    "        break\n",
    "        \n",
    "print('Links and streets are ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f00c56-4169-4780-9c24-c7549e1e9468",
   "metadata": {},
   "source": [
    "### 3 step. Clean the list of streets of unnecessary words and add \"Кокшетау\" at the end of each street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e392b-9575-403f-aae3-1702e9af5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "cleaned_streets = [\n",
    "    street.lower().replace('мкр.', '').replace('мкр', '').replace('м-н', '').replace('м.', '').strip().split(' —')[0].capitalize() + ', Кокшетау' \n",
    "    for street in streets\n",
    "]\n",
    "cleaned_streets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f47be-733f-46db-9f1a-6f779047e93a",
   "metadata": {},
   "source": [
    "### 4 step. Find all coordinates(latitude, longitude) of streets that we parsed in the step 2 and check whether the coordinates are in Kokshetau or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9d97e-c967-479c-897e-f033e95a687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"kokshetau_mapper\")\n",
    "coordinates = []\n",
    "\n",
    "for street in cleaned_streets:\n",
    "    try:\n",
    "        location = geolocator.geocode(street)\n",
    "        if location:\n",
    "            coordinates.append({\n",
    "                \"street\": street,\n",
    "                \"latitude\": location.latitude,\n",
    "                \"longitude\": location.longitude\n",
    "            })\n",
    "            print(f\"Street: {street}, latitude: {location.latitude}, longitude: {location.longitude}\")\n",
    "        else:\n",
    "            print(f\"We can't find coordinates for {street}\")\n",
    "            coordinates.append({\n",
    "                \"street\": street,\n",
    "                \"latitude\": None,\n",
    "                \"longitude\": None\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Fail to {street}: {e}\")\n",
    "        coordinates.append({\n",
    "            \"street\": street,\n",
    "            \"latitude\": None,\n",
    "            \"longitude\": None\n",
    "        })\n",
    "    time.sleep(1) \n",
    "    \n",
    "def check_city(latitude, longitude):\n",
    "    min_lat, max_lat = 53.25, 53.35\n",
    "    min_lon, max_lon = 69.35, 69.45\n",
    "\n",
    "    if min_lat <= latitude <= max_lat and min_lon <= longitude <= max_lon:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df1 = pd.DataFrame(coordinates) \n",
    "df1['is_in_kokshetau'] = df1.apply(lambda row: check_city(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145167de-3535-4491-9145-2f7578449f9f",
   "metadata": {},
   "source": [
    "### 5 step. Parsing all necessary data from ads by the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947dd87-6d6b-49f8-a7e5-417d2a4d510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=[\n",
    "    'price', 'room', 'area', 'flat_toilets', 'balcony', 'current_floors', \n",
    "    'total_floors', 'ceiling', 'dorm', 'mortgage', 'year', 'type_of_house', \n",
    "    'condition', 'repair_status', 'type_of_floor'\n",
    "])\n",
    "\n",
    "count = 0  \n",
    "\n",
    "for link in links_part1:\n",
    "\n",
    "    page_source = requests.get(\"https://krisha.kz/\" + link)\n",
    "    soup = BeautifulSoup(page_source.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        price = soup.find('div', class_=\"offer__price\").text.strip()\n",
    "        clean_price = ''.join(char for char in price if char.isdigit())\n",
    "    except:\n",
    "        clean_price = np.nan\n",
    "\n",
    "    try:\n",
    "        room = soup.find(\"h1\").text.strip()\n",
    "        match = re.search(r\"(\\d+)-комнатная\", room)\n",
    "        if match:\n",
    "            room = int(match.group(1))\n",
    "        else:\n",
    "            room = np.nan\n",
    "    except Exception as e:\n",
    "        room = np.nan  \n",
    "\n",
    "    try:\n",
    "        area = soup.find('div', {'data-name': 'live.square'})\n",
    "        area_text = area.find('div', class_='offer__advert-short-info').text\n",
    "        area_text = area_text.replace(' м²', '').strip()\n",
    "        if 'Площадь кухни' in area_text:\n",
    "            area_text = area_text.split(',')[0].strip()\n",
    "    except:\n",
    "        area_text = np.nan\n",
    "\n",
    "    try:\n",
    "        flat_toilet = soup.find('div', {'data-name': 'flat.toilet'})\n",
    "        flat_toilet_text = flat_toilet.find('div', class_=\"offer__advert-short-info\").text.strip()\n",
    "    except:\n",
    "        flat_toilet_text = np.nan\n",
    "\n",
    "    try:\n",
    "        balcony = soup.find('div', {'data-name': 'flat.balcony'})\n",
    "        balcony_text = balcony.find('div', class_=\"offer__advert-short-info\").text.strip()\n",
    "    except:\n",
    "        balcony_text = np.nan\n",
    "\n",
    "    try:\n",
    "        floor = soup.find('div', {'data-name': 'flat.floor'})\n",
    "        floor_text = floor.find('div', class_=\"offer__advert-short-info\").text.strip()\n",
    "        if ' из ' in floor_text:\n",
    "            current_floor, total_floors = map(int, floor_text.split(' из ')[0:2])\n",
    "        else:\n",
    "            current_floor, total_floors = np.nan, np.nan\n",
    "    except:\n",
    "        current_floor, total_floors = np.nan, np.nan\n",
    "\n",
    "    try:\n",
    "        ceiling = soup.find('dt', {'data-name': 'ceiling'})\n",
    "        if ceiling:\n",
    "            ceiling_text = ceiling.find_next_sibling('dd').text.strip()\n",
    "            match = re.search(r'\\d+(\\.\\d+)?', ceiling_text)\n",
    "            if match:\n",
    "                clean_ceiling = float(match.group())\n",
    "            else:\n",
    "                clean_ceiling = np.nan\n",
    "        else:\n",
    "            clean_ceiling = np.nan\n",
    "    except AttributeError:\n",
    "        clean_ceiling = np.nan\n",
    "\n",
    "    try:\n",
    "        mortgage_text = soup.find('div', class_='offer__parameters-mortgaged').text.strip()\n",
    "    except:\n",
    "        mortgage_text = np.nan\n",
    "\n",
    "    try:\n",
    "        year = soup.find('div', {'data-name': 'house.year'})\n",
    "        year_text = year.find('div', class_=\"offer__advert-short-info\").text.strip()\n",
    "    except:\n",
    "        year_text = np.nan\n",
    "\n",
    "    try:\n",
    "        type_home = soup.find('div', {'data-name': 'flat.building'})\n",
    "        type_home_text = type_home.find('div', class_=\"offer__advert-short-info\").text.strip().split('\\n')[0]\n",
    "    except:\n",
    "        type_home_text = np.nan\n",
    "\n",
    "    try:\n",
    "        dorm = soup.find('dt', {'data-name': 'flat.priv_dorm'})\n",
    "        if dorm:\n",
    "            dorm_text = dorm.find_next_sibling('dd').text.strip()\n",
    "        else:\n",
    "            dorm_text = np.nan\n",
    "    except AttributeError:\n",
    "        dorm_text = np.nan\n",
    "\n",
    "    try:\n",
    "        condition = soup.find('div', {'data-name': 'flat.renovation'})\n",
    "        condition_text = condition.find('div', class_=\"offer__advert-short-info\").text.strip()\n",
    "    except:\n",
    "        condition_text = np.nan\n",
    "\n",
    "    try:\n",
    "        repair_status = soup.find('dt', {'data-name': 'live.furniture'})\n",
    "        if repair_status:\n",
    "            repair_status_text = repair_status.find_next_sibling('dd').text.strip()\n",
    "        else:\n",
    "            repair_status_text = np.nan\n",
    "    except AttributeError:\n",
    "        repair_status_text = np.nan\n",
    "\n",
    "    try:\n",
    "        type_of_floor = soup.find('dt', {'data-name': 'flat.flooring'})\n",
    "        if type_of_floor:\n",
    "            type_of_floor_text = type_of_floor.find_next_sibling('dd').text.strip()\n",
    "        else:\n",
    "            type_of_floor_text = np.nan\n",
    "    except AttributeError:\n",
    "        type_of_floor_text = np.nan\n",
    "\n",
    "    row = [\n",
    "        clean_price, room, area_text, flat_toilet_text, balcony_text, \n",
    "        current_floor, total_floors, clean_ceiling, dorm_text, mortgage_text, \n",
    "        year_text, type_home_text, condition_text, repair_status_text, type_of_floor_text\n",
    "    ]\n",
    "    print(row)\n",
    "    df2.loc[len(df)] = row\n",
    "\n",
    "    count += 1\n",
    "    print(f\"Scraped objects: {count}\")  \n",
    "\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f6637-76df-4802-804e-a73c220aaf0f",
   "metadata": {},
   "source": [
    "### 6 step. Combining coordinate data and apartment data into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e68ccc-85a4-48ce-b6cd-07ae8903a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "df = pd.concat([df2, df1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f39fb-1ede-4808-943a-a71e61143b80",
   "metadata": {},
   "source": [
    "### 7 step. Upload all data that i scrapped into csv and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51051691-37d0-4aed-ae11-e64f20ded122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "\n",
    "with open('links_krisha_kokshetau.txt', 'w') as file:\n",
    "    for link in links:\n",
    "        file.write(link + '\\n') \n",
    "        \n",
    "df1.to_csv('coordinate_apartment_kokshetau.csv')\n",
    "df2.to_csv('dirty_krisha_data.csv')\n",
    "df.to_csv('dirty_krisha_data_with_coord_apartmnt.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
